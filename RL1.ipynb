{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Lesson #1 Q-Learning\n",
    "\n",
    "\n",
    "## Markov Decision Process Components \n",
    "\n",
    "1. **Agent**: Part of the system that is actually going to take actions and actively effect the environment.\n",
    "1. **Environment**: Part of the system that is responsibel for the effect of the actions that an agent takes\n",
    "1. **Action**: The action an agent takes\n",
    "1. **Reward**: The reqard the agent recives from the environment based on the action the agent takes\n",
    "1. **State**: Contains all the information from the environment that is needed to take an action (current action only depends on current state)\n",
    "\n",
    "### Block Diagram\n",
    "\n",
    "![Markov Decision Process](https://miro.medium.com/max/1000/1*4EYA7briZGjnnhqct-tgXw.png \"Markov Decision Process\")\n",
    "\n",
    "\n",
    "## Expected Return\n",
    "\n",
    "The total reward expected from a state at time t is given by:\n",
    "\n",
    "$$ G_t = R_{t+1} + R_{t+2} + \\cdots + R_T $$\n",
    "\n",
    "where $T$ is the final time step.\n",
    "\n",
    "\n",
    "## Discounted Expected Return\n",
    "\n",
    "Discount the effect of future rewards so as to indicate that immediate rewards have more importance as compared to rewards obtained far into the future.\n",
    "\n",
    "$$ \n",
    "\\begin{split}\n",
    "    G_t & = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma ^ n R_T \\\\\n",
    "    & = \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{t+k+1} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the discount such that $0 \\leq \\gamma \\leq 1$\n",
    "\n",
    "$$ \\gamma = 0 \\Rightarrow Value only immediate reward only $$\n",
    "\n",
    "Recurrsive equation:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma G_{t+1} $$\n",
    "\n",
    "\n",
    "## Policy Function\n",
    "\n",
    "Probability distribution of an agent taking an action $a$ given a state $s$ is called a policy.\n",
    "\n",
    "This means that at time $t$, under the policy $\\pi$, the probability of the agent taking the action $a$ given that it is in state $s$ is equal to $\\pi(a|s)$. \n",
    "\n",
    "The goal is to find the optimal policy, more on this latter.\n",
    "\n",
    "\n",
    "## State-Value Function\n",
    "\n",
    "This tells us how good a state is for an agent while the agent is following a given policy.\n",
    "\n",
    "$$ v_\\pi(s) = E \\: [G_t | S_t = s] $$\n",
    "\n",
    "## Action-Value Function\n",
    "\n",
    "This tells us how good a given action a is from $ a $ state $ s $ while the agent is following policy $ \\pi $\n",
    "\n",
    "$$ q_\\pi(s,a) = E \\: [G_t | S_t = s, A_t = a] $$\n",
    "\n",
    "## Optiomal Policy\n",
    "\n",
    "A policy such that the reward obtained by following this policy is equal to or greater that the reward that can be achieved by following any other policy.\n",
    "\n",
    "## Optimal State-Value Function\n",
    "\n",
    "$$ v_*(s) = max \\; v_\\pi(s) \\; \\forall \\; \\pi $$\n",
    "\n",
    "\n",
    "## Optimal Action-Value Function\n",
    "\n",
    "$$ q_*(s, a) = max \\; q_\\pi(s, a) \\; \\forall \\; \\pi $$\n",
    "\n",
    "## Bellmam Optimality  for $q_*$\n",
    "\n",
    "$$ q_*(s, a) = E \\: [R_{t+1} + \\gamma max q_*(s', a')] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.001\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Average rewards per thousand rewards*****\n",
      "1000: 0.04900000000000004\n",
      "2000: 0.20800000000000016\n",
      "3000: 0.4310000000000003\n",
      "4000: 0.5790000000000004\n",
      "5000: 0.6770000000000005\n",
      "6000: 0.7100000000000005\n",
      "7000: 0.6850000000000005\n",
      "8000: 0.7270000000000005\n",
      "9000: 0.7380000000000005\n",
      "10000: 0.7240000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}', end='\\r')\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 1000)\n",
    "count = 1000\n",
    "print('*****Average rewards per thousand rewards*****')\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(f'{count}: {sum(r/1000)}')\n",
    "    count += 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "*****You Reached Your Goal*****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(f'*****Episode {episode + 1}*****')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print('*****You Reached Your Goal*****')\n",
    "            else:\n",
    "                print('*****Fell Through A Hole')\n",
    "            time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
