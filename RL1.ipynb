{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Lesson #1 Q-Learning\n",
    "\n",
    "\n",
    "## Markov Decision Process Components \n",
    "\n",
    "1. **Agent**: Part of the system that is actually going to take actions and actively effect the environment.\n",
    "1. **Environment**: Part of the system that is responsibel for the effect of the actions that an agent takes\n",
    "1. **Action**: The action an agent takes\n",
    "1. **Reward**: The reqard the agent recives from the environment based on the action the agent takes\n",
    "1. **State**: Contains all the information from the environment that is needed to take an action (current action only depends on current state)\n",
    "\n",
    "### Block Diagram\n",
    "\n",
    "![Markov Decision Process](https://miro.medium.com/max/1000/1*4EYA7briZGjnnhqct-tgXw.png \"Markov Decision Process\")\n",
    "\n",
    "\n",
    "## Expected Return\n",
    "\n",
    "The total reward expected from a state at time t is given by:\n",
    "\n",
    "$$ G_t = R_{t+1} + R_{t+2} + \\cdots + R_T $$\n",
    "\n",
    "where $T$ is the final time step.\n",
    "\n",
    "\n",
    "## Discounted Expected Return\n",
    "\n",
    "Discount the effect of future rewards so as to indicate that immediate rewards have more importance as compared to rewards obtained far into the future.\n",
    "\n",
    "$$ \n",
    "\\begin{split}\n",
    "    G_t & = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma ^ n R_T \\\\\n",
    "    & = \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{t+k+1} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the discount such that $0 \\leq \\gamma \\leq 1$\n",
    "\n",
    "$$ \\gamma = 0 \\Rightarrow Value only immediate reward only $$\n",
    "\n",
    "Recurrsive equation:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma G_{t+1} $$\n",
    "\n",
    "\n",
    "## Policy Function\n",
    "\n",
    "Probability distribution of an agent taking an action $a$ given a state $s$ is called a policy.\n",
    "\n",
    "This means that at time $t$, under the policy $\\pi$, the probability of the agent taking the action $a$ given that it is in state $s$ is equal to $\\pi(a|s)$. \n",
    "\n",
    "The goal is to find the optimal policy, more on this latter.\n",
    "\n",
    "\n",
    "## State-Value Function\n",
    "\n",
    "This tells us how good a state is for an agent while the agent is following a given policy.\n",
    "\n",
    "$$ v_\\pi(s) = E \\: [G_t | S_t = s] $$\n",
    "\n",
    "## Action-Value Function\n",
    "\n",
    "This tells us how good a given action a is from $ a $ state $ s $ while the agent is following policy $ \\pi $\n",
    "\n",
    "$$ q_\\pi(s,a) = E \\: [G_t | S_t = s, A_t = a] $$\n",
    "\n",
    "## Optiomal Policy\n",
    "\n",
    "A policy such that the reward obtained by following this policy is equal to or greater that the reward that can be achieved by following any other policy.\n",
    "\n",
    "## Optimal State-Value Function\n",
    "\n",
    "$$ v_*(s) = max \\; v_\\pi(s) \\; \\forall \\; \\pi $$\n",
    "\n",
    "\n",
    "## Optimal Action-Value Function\n",
    "\n",
    "$$ q_*(s, a) = max \\; q_\\pi(s, a) \\; \\forall \\; \\pi $$\n",
    "\n",
    "## Bellmam Optimality  for $q_*$\n",
    "\n",
    "$$ q_*(s, a) = E \\: [R_{t+1} + \\gamma max q_*(s', a')] $$\n",
    "\n",
    "\n",
    "## Update Rule\n",
    "\n",
    "$$ q_*(s,a) = (1-\\alpha) q_*(s,a) + \\alpha (R_{t+1} + \\gamma max q_*(s', a')) $$\n",
    "\n",
    "where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set Random Seed to get consistant results on every run\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of possible states and actions\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Function (initialize with zeros)\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameter Values\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.001\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Average rewards per thousand rewards*****\n",
      "1000: 0.05300000000000004\n",
      "2000: 0.20400000000000015\n",
      "3000: 0.44000000000000034\n",
      "4000: 0.6020000000000004\n",
      "5000: 0.6580000000000005\n",
      "6000: 0.7100000000000005\n",
      "7000: 0.7060000000000005\n",
      "8000: 0.7410000000000005\n",
      "9000: 0.7490000000000006\n",
      "10000: 0.7330000000000005\n"
     ]
    }
   ],
   "source": [
    "# TRAINING BLOCK\n",
    "\n",
    "# Array to store all final rewards\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# Loop over all the episodes\n",
    "for episode in range(num_episodes):\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}', end='\\r')\n",
    "    \n",
    "#     Reset Environment before each episode begins\n",
    "    state = env.reset()\n",
    "    \n",
    "#     done keeps track of whether or not the episode has ended\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "#     Run each step in an wpisode\n",
    "    for step in range(max_steps_per_episode):\n",
    "#         Get a random exploration threshold\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "#         Exploitation\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "#         Exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "#         Take the action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "#         Update the Q-Function\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "#         If episode is over break out of this loop\n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "#     Update Exploration Rate\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Display average per 1000 episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 1000)\n",
    "count = 1000\n",
    "print('*****Average rewards per thousand rewards*****')\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(f'{count}: {sum(r/1000)}')\n",
    "    count += 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53388668, 0.50793803, 0.50348681, 0.50213563],\n",
       "       [0.37778246, 0.28058943, 0.23027297, 0.51798619],\n",
       "       [0.4206554 , 0.39881638, 0.42140322, 0.48766767],\n",
       "       [0.33335938, 0.3012729 , 0.33662116, 0.4763035 ],\n",
       "       [0.55585494, 0.45640838, 0.35563161, 0.35649225],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.32759623, 0.19038875, 0.19602096, 0.13854938],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.47956199, 0.41481605, 0.43261246, 0.58338623],\n",
       "       [0.33569352, 0.65063026, 0.35098708, 0.3002485 ],\n",
       "       [0.65772026, 0.4171163 , 0.44687961, 0.32233445],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.49919183, 0.53281897, 0.76274494, 0.54849327],\n",
       "       [0.74046701, 0.83259489, 0.72204179, 0.72513907],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the learned Q-Function\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won 2/3 times.\n"
     ]
    }
   ],
   "source": [
    "# DEMO BLOCK\n",
    "count = 0\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(f'*****Episode {episode + 1}*****')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                count += 1\n",
    "                print('*****You Reached Your Goal*****')\n",
    "            else:\n",
    "                print('*****Fell Through A Hole')\n",
    "            time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "env.close()\n",
    "print(f'Won {count}/3 times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
